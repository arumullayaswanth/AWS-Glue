# 🚀 AWS Glue: End-to-End ETL Walkthrough

This guide reproduces the steps from the "Getting started with AWS Glue | Hands‑On | Basic end‑to‑end" 

### end-to-end transformation pipeline / aws-glue-csv-to-parquet-pipeline
1. This name reflects the main functionality:
  - Using AWS Glue
  - Reading CSV files
  - Transforming and writing to Parquet
  - As part of a data pipeline
  - Using Python as the language

### ✅ Topics to Use in Your AWS Glue Project
            - aws
            - aws-glue
            - etl
            - data-engineering
            - s3
            - aws-glue-crawler
            - data-pipeline
            - pyspark
            - big-data
            - aws-athena
            - data-lake
            - serverless
            - parquet
            - data-transformation
            - cloud-computing




---

## 1. Define IAM Role for Glue
- Go to **IAM > Roles > Create role**.
- choose **AWS service** as the trusted entity.
- Choose **Glue** as the trusted entity.
- check the **AWS Glue** box.
- Next: **Attach policies**.
  - `AmazonS3FullAccess` or limited access to your buckets
  - `AWSGlueServiceRole`
- Next: **Review and create role**.  
- Name the role, e.g., `GlueDemoRole`, and create it.


## 2. Setup: Create S3 Input & Output Buckets


### 📁 S3 Folder Structure

```
my-glue-demo-input/
├── data-store/
│   └── customers_report/
│       └── customers.csv
│
├── target-datas-store/
│   └── parquet_report/
│       └── output.parquet  (generated by AWS Glue)
```


### step 1:
- Navigate to **Amazon S3** in the AWS Console.
- Create buckets:
- Bucket Name:
  - `my-glue-demo-input`
- AWS Region: `us-east-1`
- Object Ownership: **Enabled ACLs disabled (recommended)**
- Block public access: **select Block all public access**
- bucket versioning: **Enabled**
- Next: **Create bucket**.

### setp 2:
- open the `my-glue-demo-input`bucket, and create folders:
  - Folder Name: `data-store`
  - clic **Create folder**

- open `data-store` folders and click one more folder:
  - Folder Name: `customers_report`
  - clic **Create folder**
  - Upload CSV files (e.g., `customers.csv`) 

### setp 2:
NOTE: we will also create the database with sane name that is `customers_report` however it not necessary that we need to create the database with the same name. 
- it's just that it is easy to map things with this kind of naming convention.

- `customers_report` this is our databasere now within this database we will create the table.(just logically telling how we will be mapping the data with the database)

- open `customers_report` folders and click one more folder:
    - Folder Name: `customers_csv_reporst`
   - clic **Create folder**
   - Upload CSV files (e.g., `customers.csv`) into the **input** bucket.

### step 3:
1. Navigate to select  `customers.csv` go to Actions and click **Query with S3 select**
    - input settings 
    - format **CSV**
    - CSV delimiter: **comma**
    - compression type: **None**
    - Output settings
    - format **csv**
    - CSV delimiter: **comma**

2. SQL query
      - here we have a smaple query so we are not going to modify it. 
      - (Example query)
      ```sql
      SELECT *
      FROM s3object
      WHERE s3object.key LIKE '%customers.csv'
      ```

3. query results
      - status: successfully returned
      - click Formatted results
      - **Note:** we are not going to download the results. 
      how the data looks like in the database. so here we have multiple columns and rows. 
      - **Note:** now we want to transform the data and write it to the parquet file format. 
- click **close** or move back to our s3 bucket

### step 4:  Output Buckets
- open the `my-glue-demo-input`bucket, and create folders:
  - Folder Name: `target-datas-store`
  - clic **Create folder**
  **Note**: I want to store the packet file in this derectory

- open `target-datas-store` folders and click one more folder:
  - Folder Name: `parquet_report`
  - clic **Create folder**

**NOTE**: hwre we sussessfully configured th S3 bucket along with its folders structure and next step will navigate to the aws glue managment console.

---



## 3. create Database
- Open **AWS Glue > Data Catalog > Databases**.
  1. create database `customers_report`
  - Note: we are going to provide the location of this particular database in thes3 bucker o simply we are going to copy and paste the URI ot that data store `customers_report` select the `customers_report` and copy the URI and paste in the next step.
  2. location : s3://
  3. Click **Create database**.

Note : now i crate the database `customers_report` and the next step is to create the table in the database. but here we are going to user crawler to create the table for me

## 4. craate a table
- Navigate to **AWS Glue > Data Catalog > tables**.
- add table using crawler 
     - name : `customers_report_crawler`
     - next
        - choose data soursr and classifiers
           - Data source configuration
                - is your data alreads mapped to glue table (**Not yet**)
                - Data source > Add a data source
                  - Data source **s3**
                  - location of s3 data **In this account**
                  - s3 Path
                  - `s3://my-glue-demo-input/customers_report/customers.csv/
                  **Note: make sure you add the forward slash(/_)**
                  - Add an s3 data source
                  - Next
        - configure security settings
            - EXisting IAM role : `GlueDemoRole`
            - next

        - set output and scheduling
            - output configuration
                - target database : `customers_report`
                - crawler schedule
                     - frequency : `on demand'
                     - next
            - Review and create         
          - create crawler
**note**: now i create the table `customers_report_crawler` and select and then **RUN**
        - Now it will take some time depending on the data that we have in the s3 bucket

## 5. create the ETL job
- Open **AWS Glue > Data lnterface and ETL >AWS Glue Studio > jobs**.
- create job > select >**visual with a blank canvas**
- create
1. - souece : **select AWS Glue Data catalog**
   - in left side click on **Data source properties - Data catalog**
   - select Database : `customers_report`
   - table : `customers_report_crawler`
   - next
   - note: we have successfully configured the source that in aws glue data catalog and now we need to apply some sort of transformation to this data.

2. - transform  : **select Apply Mapping**
   - in left side click on **Apply mapping**
   - select **Apply mapping**
   - next
   - note: we have successfully configured the transform tan now finally we want to load the dsta somewhere so select this applied mapping

3. - target : **select Amazon s3**
   - in left side click on **Data target properties -s3**
   - format : **Parquet**
   - compression type : **uncompressed**
   - s3 target location : `s3://my-glue-demo-output/target-data-store/parquet-report-crawler/`
   - Data catalog update options : **Do not update the Data catalog**
   - next
   - note: we have successfully configured the Data source the transformation and Data traget

4. click Script
   - so here you should be able to see the script that aws glue has generated for us that howw this transformate will take place.

5. click job details
   - **job name**: `customers_report_etl`
   - **job role**: `GlueDemoRole`
   - go to top right click and select **save**
   - **Run**: **Run job** 

6. click Rum
     - you can see the status
     - you can check the logs and metrics in **Glue Studio** or **CloudWatch logs**.
     - **Job status**: `Succeeded`
     - **Job duration**: `1


---





